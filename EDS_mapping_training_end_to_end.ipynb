{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import albumentations as albu\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib\n",
    "import scipy\n",
    "from segmentation_models_pytorch import utils as smp_utils\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "from natsort import natsorted\n",
    "from torchviz import make_dot\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_annot')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'train_annot')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'train_annot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(image, mask):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = mask.shape[2]+1\n",
    "    plt.figure(figsize=(n*6, 5))\n",
    "    # for i, (name, image) in enumerate(images.items()):\n",
    "\n",
    "    plt.subplot(1, n, 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # plt.title(' '.join(name.split('_')).title())\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    cmaps = [\n",
    "        matplotlib.colors.ListedColormap(['black', 'red']),\n",
    "        matplotlib.colors.ListedColormap(['black', 'orange']),\n",
    "        matplotlib.colors.ListedColormap(['black', 'blue']),\n",
    "        matplotlib.colors.ListedColormap(['black', 'pink']),\n",
    "        matplotlib.colors.ListedColormap(['black', 'green']),\n",
    "        matplotlib.colors.ListedColormap(['black', 'yellow']),\n",
    "        matplotlib.colors.ListedColormap(['black', 'red']),\n",
    "    ]\n",
    "    names = ['C', 'Ca', 'Mg', 'Na', 'O', 'S', 'Cl']\n",
    "    \n",
    "    for j in range(7):\n",
    "        plt.subplot(1, n, j + 2)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        name = names[j]\n",
    "        plt.title(name)\n",
    "        plt.imshow(mask[:, :, j], cmap = cmaps[j], vmin=0, vmax=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, images_dir, masks_dir, augmentation=None, preprocessing=None):\n",
    "        # list file names in the self.ids list\n",
    "        self.sem_ids = os.listdir(images_dir)\n",
    "        self.label_ids = os.listdir(masks_dir)\n",
    "        \n",
    "        self.sem_ids.sort()\n",
    "        self.label_ids.sort()\n",
    "        \n",
    "        if self.sem_ids[0].startswith('.'):\n",
    "            self.sem_ids.pop(0)\n",
    "            \n",
    "        if self.label_ids[0].startswith('.'):\n",
    "            self.label_ids.pop(0)\n",
    "        \n",
    "        self.sem_ids = natsorted(self.sem_ids)\n",
    "        self.label_ids = natsorted(self.label_ids)\n",
    "        \n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.sem_ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, label_id) for label_id in self.label_ids]\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        # print(image.shape)\n",
    "        # converting the file dimension in [N, C, H, W] order\n",
    "        # image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mat = scipy.io.loadmat(self.masks_fps[i])\n",
    "        mask = mat['label']\n",
    "        mask = np.transpose(mask, (1, 2, 0))\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sem_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    \n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        # albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        # albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        # albu.IAAPerspective(p=0.5),\n",
    "        albu.Perspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                # albu.RandomBrightness(p=1),\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                # albu.IAASharpen(p=1),\n",
    "                albu.Sharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                # albu.RandomContrast(p=1),\n",
    "                albu.RandomBrightnessContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        # albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\", \n",
    "    encoder_weights='imagenet',\n",
    "    classes = 7, \n",
    "    activation='sigmoid'\n",
    ")\n",
    "\n",
    "file_exists = exists('best_model.pth')\n",
    "# if file_exists:\n",
    "#     model = torch.load('./best_model.pth')\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_dataset = Dataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset without transformations for image visualization\n",
    "test_dataset_vis = Dataset(\n",
    "    x_test_dir, y_test_dir, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = nn.Sequential(\n",
    "    model,\n",
    "    nn.Conv2d(7, 7, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(4, 4),\n",
    "    nn.Conv2d(7, 7, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(4, 4),\n",
    "    nn.Conv2d(7, 7, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 1920, 2560]) torch.Size([4, 7, 1920, 2560])\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "for source, targets in train_loader:\n",
    "    print(source.shape, targets.shape)\n",
    "    # outputs = model(source)\n",
    "    outputs = new_model(source)\n",
    "    print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = smp.losses.DiceLoss(\"multilabel\")\n",
    "loss.__name__ = 'Dice_loss'\n",
    "\n",
    "metrics = [\n",
    "    smp_utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.01),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp_utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "valid_epoch = smp_utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "\n",
    "#################################\n",
    "n = np.random.choice(len(train_dataset))\n",
    "\n",
    "train_dataset_vis = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source, targets = next(iter(train_loader))\n",
    "# source = source.to(DEVICE)\n",
    "# yhat = model(source)\n",
    "# make_dot(yhat, params=dict(list(model_regression.named_parameters()))).render(\"EDS_mapping_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1000):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    # valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < train_logs['iou_score']:\n",
    "        max_score = train_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        # print('Iou score:', max_score)\n",
    "        print('Model saved!')\n",
    "        \n",
    "    # if i == 25:\n",
    "    #     optimizer.param_groups[0]['lr'] = 1e-5\n",
    "    #     print('Decrease decoder learning rate to 1e-5!')\n",
    "        \n",
    "    if i % 200 == 0:\n",
    "        n = np.random.choice(len(test_dataset))\n",
    "        image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "        image, gt_mask = test_dataset[n]\n",
    "        # image = np.transpose(image, (2, 0, 1))\n",
    "        gt_mask = gt_mask.squeeze()\n",
    "        x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "        x_tensor = x_tensor.float()\n",
    "        best_model = torch.load('./best_model.pth')\n",
    "        pr_mask = best_model.predict(x_tensor)\n",
    "        pr_mask = pr_mask.squeeze().cpu().numpy().round()\n",
    "        pr_mask = np.transpose(pr_mask, (1, 2, 0))\n",
    "\n",
    "        gt_mask = np.transpose(gt_mask, (1, 2, 0))\n",
    "        visualize(\n",
    "            image=image_vis, \n",
    "            mask=gt_mask,\n",
    "        )\n",
    "        visualize(\n",
    "            image=image_vis, \n",
    "            mask=pr_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDS_predict_path = 'EDS_predict_path'\n",
    "\n",
    "try: \n",
    "    os.mkdir(EDS_predict_path)\n",
    "except OSError as error: \n",
    "    print(error)  \n",
    "\n",
    "EDS_train_dir = os.path.join(DATA_DIR, 'EDS_predict_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(test_dataset)):\n",
    "for i in range(5):\n",
    "    for j in range(25):\n",
    "        for k in range(5):\n",
    "            image, gt_mask = test_dataset[i]\n",
    "            # image = np.transpose(image, (2, 0, 1))\n",
    "            gt_mask = gt_mask.squeeze()\n",
    "            x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "            x_tensor = x_tensor.float()\n",
    "            best_model = torch.load('./best_model.pth')\n",
    "            pr_mask = best_model.predict(x_tensor)\n",
    "            pr_mask = pr_mask.squeeze().cpu().numpy().round()\n",
    "            image = np.transpose(image, (1, 2, 0))\n",
    "            pr_mask = np.transpose(pr_mask, (1, 2, 0))\n",
    "            # print(image.shape, pr_mask.shape) (1920, 2560, 3) (1920, 2560, 7)\n",
    "            image_mask = np.concatenate((image, pr_mask), axis=2)\n",
    "            np.save(EDS_predict_path+'/'+str(i)+'_'+str(j)+'_'+str(k)+'.npy', image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set\n",
    "test_epoch = smp_utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = test_epoch.run(test_dataloader)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcium = pd.read_csv(r'ele_cons/Calcium.csv', header=None)\n",
    "carbon = pd.read_csv(r'ele_cons/Carbon.csv', header=None)\n",
    "chlorine = pd.read_csv(r'ele_cons/Chlorine.csv', header=None)\n",
    "magnesium = pd.read_csv(r'ele_cons/Magnesium.csv', header=None)\n",
    "oxygen = pd.read_csv(r'ele_cons/Oxygen.csv', header=None)\n",
    "sodium = pd.read_csv(r'ele_cons/Sodium.csv', header=None)\n",
    "sulphur = pd.read_csv(r'ele_cons/Sulphur.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcium = np.reshape(calcium.to_numpy(), (125, 1))\n",
    "carbon = np.reshape(carbon.to_numpy(), (125, 1))\n",
    "chlorine = np.reshape(chlorine.to_numpy(), (125, 1))\n",
    "magnesium = np.reshape(magnesium.to_numpy(), (125, 1))\n",
    "oxygen = np.reshape(oxygen.to_numpy(), (125, 1))\n",
    "sodium = np.reshape(sodium.to_numpy(), (125, 1))\n",
    "sulphur = np.reshape(sulphur.to_numpy(), (125, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcium = np.repeat(calcium, 5)\n",
    "carbon = np.repeat(carbon, 5)\n",
    "chlorine = np.repeat(chlorine, 5)\n",
    "magnesium = np.repeat(magnesium, 5)\n",
    "oxygen = np.repeat(oxygen, 5)\n",
    "sodium = np.repeat(sodium, 5)\n",
    "sulphur = np.repeat(sulphur, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcium = np.reshape(calcium, (625, 1))\n",
    "carbon = np.reshape(carbon, (625, 1))\n",
    "chlorine = np.reshape(chlorine, (625, 1))\n",
    "magnesium = np.reshape(magnesium, (625, 1))\n",
    "oxygen = np.reshape(oxygen, (625, 1))\n",
    "sodium = np.reshape(sodium, (625, 1))\n",
    "sulphur = np.reshape(sulphur, (625, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = np.concatenate((calcium, carbon, chlorine, magnesium, oxygen, sodium, sulphur), axis=1)\n",
    "np.save('elements.npy', elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from utils import MyTrainDataset\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ddp_setup(rank, world_size):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         rank: Unique identifier of each process\n",
    "#         world_size: Total number of processes\n",
    "#     \"\"\"\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "#     init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model: torch.nn.Module,\n",
    "#         train_data: DataLoader,\n",
    "#         optimizer: torch.optim.Optimizer,\n",
    "#         gpu_id: int,\n",
    "#         save_every: int,\n",
    "#     ):\n",
    "#         self.gpu_id = gpu_id\n",
    "#         self.model = model.to(gpu_id)\n",
    "#         self.train_data = train_data\n",
    "#         self.optimizer = optimizer\n",
    "#         self.save_every = save_every\n",
    "#         self.model = DDP(model, device_ids=[gpu_id])\n",
    "\n",
    "#     def _run_batch(self, source, targets):\n",
    "#         self.optimizer.zero_grad()\n",
    "#         output = self.model(source)\n",
    "#         loss = F.L1_LOSS(output, targets)\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#     def _run_epoch(self, epoch):\n",
    "#         b_sz = len(next(iter(self.train_data))[0])\n",
    "#         print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
    "#         self.train_data.sampler.set_epoch(epoch)\n",
    "#         for source, targets in self.train_data:\n",
    "#             source = source.to(self.gpu_id)\n",
    "#             targets = targets.to(self.gpu_id)\n",
    "#             self._run_batch(source, targets)\n",
    "\n",
    "#     def _save_checkpoint(self, epoch):\n",
    "#         ckp = self.model.module.state_dict()\n",
    "#         PATH = \"checkpoint.pt\"\n",
    "#         torch.save(ckp, PATH)\n",
    "#         print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
    "\n",
    "#     def train(self, max_epochs: int):\n",
    "#         for epoch in range(max_epochs):\n",
    "#             self._run_epoch(epoch)\n",
    "#             if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
    "#                 self._save_checkpoint(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, elements):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_files = os.listdir(data_dir)\n",
    "        self.data_files.sort()\n",
    "        if self.data_files[0].startswith('.'):\n",
    "            self.data_files.pop(0)\n",
    "        \n",
    "        self.elements = np.load(elements)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.data_dir + '/' + self.data_files[idx]) # (1920, 2560, 10)\n",
    "        data = np.transpose(data, (2, 0, 1))\n",
    "        elements = self.elements[idx]\n",
    "        elements = np.float32(elements)\n",
    "        return torch.tensor(data), torch.tensor(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('EDS_predict_path', 'elements.npy')\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(10, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc1 = nn.Linear(64 * 477 * 637, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 64 * 477 * 637)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regression = TheModelClass()\n",
    "model_regression = model_regression.to('cuda')\n",
    "print(model_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_regression.parameters(), lr=1e-3)\n",
    "loss_func = nn.MSELoss()\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, data_loader):\n",
    "    total_loss = 0\n",
    "    j = 0\n",
    "    for source, targets in dataloader:\n",
    "        print(j)\n",
    "        j = j + 1\n",
    "        source = source.to('cuda')\n",
    "        targets = targets.to('cuda')\n",
    "        output = model_regression(source)\n",
    "        loss = loss_func(output, targets)\n",
    "        total_loss = total_loss + loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 1000):\n",
    "    total_loss = 0\n",
    "    for source, targets in dataloader:\n",
    "        source = source.to('cuda')\n",
    "        targets = targets.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        output = model_regression(source)\n",
    "        loss = loss_func(output, targets)\n",
    "        total_loss = total_loss + loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        # total_loss = run_test(model_regression, dataloader)\n",
    "        print(f\"Epoch {epoch} | Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
