{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d9d382-0a60-44fe-ba6d-925e3dfcc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4f173c-e0f0-46e2-a99b-487885d9e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _get_layers(nn.Module):\n",
    "    def __init__(self, elements=7):\n",
    "        super().__init__()\n",
    "        self.elements = elements\n",
    "    def forward(self, x):\n",
    "        sem = x[:, :3, :, :]\n",
    "        c = x[:, 3, :, :]\n",
    "        c = c[:, None, :, :] \n",
    "        ca = x[:, 4, :, :]\n",
    "        ca = ca[:, None, :, :]\n",
    "        mg = x[:, 5, :, :]\n",
    "        mg = mg[:, None, :, :]\n",
    "        na = x[:, 6, :, :]\n",
    "        na = na[:, None, :, :]\n",
    "        o = x[:, 7, :, :]\n",
    "        o = o[:, None, :, :]\n",
    "        s = x[:, 8, :, :]\n",
    "        s = s[:, None, :, :]\n",
    "        cl = x[:, 9, :, :]\n",
    "        cl = cl[:, None, :, :]\n",
    "        return sem, c, ca, mg, na, o, s, cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c086c1c-f5b4-4f90-99ca-0dc58c4b9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d901e26-aad6-4fe3-97ab-7a3417ba6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        # a = self.q_linear(q)\n",
    "        # print(a.shape)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # print(q.shape)\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e2a74a-4477-4631-8ed0-667594e5046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "# net = FeedForward(158)\n",
    "# x=torch.randn(10, 158)\n",
    "# y = net(x)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd517a58-4246-452b-8f59-c26b4b7e7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "    \n",
    "# net = Norm(158)\n",
    "# x=torch.randn(10, 158)\n",
    "# y = net(x\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df2bb838-2304-4a03-8fa4-3347088e86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "    def forward(self, x, e_outputs, src_mask=None, trg_mask=None):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "    # We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160ed23d-8438-4d63-9000-5e13f848470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 300):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                \n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        return x\n",
    "\n",
    "# net = PositionalEncoder(282)\n",
    "# x = src = torch.randn(8, 211, 282)\n",
    "# y = net(x)\n",
    "# y.shape\n",
    "\n",
    "# net = PositionalEncoder(158)\n",
    "# x = src = torch.randn(8, 118, 158)\n",
    "# y = net(x)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b43cb27-6f01-4047-80a6-fdadba309f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        # self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask=None):\n",
    "        # x = self.embed(src)\n",
    "        x = src\n",
    "        # x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        # self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask=None, trg_mask=None):\n",
    "        # x = self.embed(trg)\n",
    "        # x = self.pe(x)\n",
    "        x = trg\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c823a3d2-6dd3-4575-8027-ca400e627265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.reduce = nn.Linear(d_model, trg_vocab)\n",
    "        # self.reduce = nn.Linear(d_model, 1)\n",
    "        self.linear_1 = nn.Linear(trg_vocab*src_vocab, trg_vocab)\n",
    "        self.out = nn.Linear(56, 1)\n",
    "    def forward(self, src, trg, src_mask=None, trg_mask=None):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.reduce(d_output)\n",
    "        output = output.view(output.shape[0], -1)        \n",
    "        output = self.linear_1(output)\n",
    "        output = output.view(-1)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee50fe57-625d-478b-b991-23c0c9e358af",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 118\n",
    "trg_vocab = 7\n",
    "# t = Transformer(src_vocab, trg_vocab, 158, 3, 2)\n",
    "# src = torch.randn(8, 118, 158)\n",
    "# trg = torch.randn(8, 118, 158)\n",
    "# # t = t.to('cuda')\n",
    "# # src = src.to('cuda')\n",
    "# # trg = trg.to('cuda')\n",
    "# output = t(src, trg)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669621f1-9753-4672-8be4-e1cc1690e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _feature_extraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_feature_extraction, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(4, 4)\n",
    "        self.conv1_sem = nn.Conv2d(3, 4, 5)\n",
    "        self.conv2_sem = nn.Conv2d(4, 8, 5)\n",
    "        self.conv1_c = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2_c = nn.Conv2d(4, 8, 5)\n",
    "        self.conv1_ca = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2_ca = nn.Conv2d(4, 8, 5)\n",
    "        self.conv1_mg = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2_mg = nn.Conv2d(4, 8, 5)\n",
    "        self.conv1_na = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2_na = nn.Conv2d(4, 8, 5)\n",
    "        self.conv1_o = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2_o = nn.Conv2d(4, 8, 5)\n",
    "        self.conv1_s = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2_s = nn.Conv2d(4, 8, 5)\n",
    "        self.conv1_cl = nn.Conv2d(1, 4, 5)\n",
    "        self.conv2_cl = nn.Conv2d(4, 8, 5)\n",
    "    def forward(self, sem, c, ca, mg, na, o, s, cl):\n",
    "        sem = self.pool(F.relu(self.conv1_sem(sem)))\n",
    "        sem = self.pool(F.relu(self.conv2_sem(sem))) #torch.Size([4, 8, 211, 282])\n",
    "        c = self.pool(F.relu(self.conv1_c(c)))\n",
    "        c = self.pool(F.relu(self.conv2_c(c))) #torch.Size([4, 8, 211, 282])\n",
    "        ca = self.pool(F.relu(self.conv1_ca(ca)))\n",
    "        ca = self.pool(F.relu(self.conv2_ca(ca))) #torch.Size([4, 8, 211, 282])\n",
    "        mg = self.pool(F.relu(self.conv1_mg(mg)))\n",
    "        mg = self.pool(F.relu(self.conv2_mg(mg))) #torch.Size([4, 8, 211, 282])\n",
    "        na = self.pool(F.relu(self.conv1_na(na)))\n",
    "        na = self.pool(F.relu(self.conv2_na(na))) #torch.Size([4, 8, 211, 282])\n",
    "        o = self.pool(F.relu(self.conv1_o(o)))\n",
    "        o = self.pool(F.relu(self.conv2_o(o))) #torch.Size([4, 8, 211, 282])\n",
    "        s = self.pool(F.relu(self.conv1_s(s)))\n",
    "        s = self.pool(F.relu(self.conv2_s(s))) #torch.Size([4, 8, 211, 282])\n",
    "        cl = self.pool(F.relu(self.conv1_cl(cl)))\n",
    "        cl = self.pool(F.relu(self.conv2_cl(cl))) #torch.Size([4, 8, 211, 282])\n",
    "        \n",
    "        return sem, c, ca, mg, na, o, s, cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53856fd5-ed8f-41bb-af8e-caf4ee670b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _cross_transformer(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(_cross_transformer, self).__init__()\n",
    "        \n",
    "        # self.transformer = Transformer(211, 7, 282, 3, 6)\n",
    "        self.transformer = Transformer(118, 7, 158, 3, 2)\n",
    "\n",
    "    def forward(self, sem, c, ca, mg, na, o, s, cl):\n",
    "        c_predict = torch.empty((1,7), dtype=torch.float32).to(device)\n",
    "        ca_predict = torch.empty((1,7), dtype=torch.float32).to(device)\n",
    "        mg_predict = torch.empty((1,7), dtype=torch.float32).to(device)\n",
    "        na_predict = torch.empty((1,7), dtype=torch.float32).to(device)\n",
    "        o_predict = torch.empty((1,7), dtype=torch.float32).to(device)\n",
    "        s_predict = torch.empty((1,7), dtype=torch.float32).to(device)\n",
    "        cl_predict = torch.empty((1,7), dtype=torch.float32).to(device)\n",
    "        \n",
    "        for i in range(sem.shape[0]):\n",
    "            \n",
    "            sem_i = sem[i]\n",
    "            c_i = c[i]\n",
    "            ca_i = ca[i]\n",
    "            mg_i = mg[i]\n",
    "            na_i = na[i]\n",
    "            o_i = o[i]\n",
    "            s_i = s[i]\n",
    "            cl_i = cl[i]\n",
    "            \n",
    "            c_sem = self.transformer(c_i, sem_i)\n",
    "            c_ca = self.transformer(c_i, ca_i)\n",
    "            c_mg = self.transformer(c_i, mg_i)\n",
    "            c_na = self.transformer(c_i, na_i)\n",
    "            c_o = self.transformer(c_i, o_i)\n",
    "            c_s = self.transformer(c_i, s_i)\n",
    "            c_cl = self.transformer(c_i, cl_i)\n",
    "            c_temp = torch.cat((c_sem, c_ca, c_mg, c_na, c_o, c_s, c_cl), 0)\n",
    "            \n",
    "            ca_sem = self.transformer(ca_i, sem_i)\n",
    "            ca_c = self.transformer(ca_i, c_i)\n",
    "            ca_mg = self.transformer(ca_i, mg_i)\n",
    "            ca_na = self.transformer(ca_i, na_i)\n",
    "            ca_o = self.transformer(ca_i, o_i)\n",
    "            ca_s = self.transformer(ca_i, s_i)\n",
    "            ca_cl = self.transformer(ca_i, cl_i)\n",
    "            ca_temp = torch.cat((ca_sem, ca_c, ca_mg, ca_na, ca_o, ca_s, ca_cl), 0)\n",
    "            \n",
    "            mg_sem = self.transformer(mg_i, sem_i)\n",
    "            mg_c = self.transformer(mg_i, c_i)\n",
    "            mg_ca = self.transformer(mg_i, ca_i)\n",
    "            mg_na = self.transformer(mg_i, na_i)\n",
    "            mg_o = self.transformer(mg_i, o_i)\n",
    "            mg_s = self.transformer(mg_i, s_i)\n",
    "            mg_cl = self.transformer(mg_i, cl_i)\n",
    "            mg_temp = torch.cat((mg_sem, mg_c, mg_ca, mg_na, mg_o, mg_s, mg_cl), 0)\n",
    "            \n",
    "            na_sem = self.transformer(na_i, sem_i)\n",
    "            na_c = self.transformer(na_i, c_i)\n",
    "            na_ca = self.transformer(na_i, ca_i)\n",
    "            na_mg = self.transformer(na_i, mg_i)\n",
    "            na_o = self.transformer(na_i, o_i)\n",
    "            na_s = self.transformer(na_i, s_i)\n",
    "            na_cl = self.transformer(na_i, cl_i)\n",
    "            na_temp = torch.cat((na_sem, na_c, na_ca, na_mg, na_o, na_s, na_cl), 0)\n",
    "            \n",
    "            o_sem = self.transformer(o_i, sem_i)\n",
    "            o_c = self.transformer(o_i, c_i)\n",
    "            o_ca = self.transformer(o_i, ca_i)\n",
    "            o_mg = self.transformer(o_i, mg_i)\n",
    "            o_na = self.transformer(o_i, na_i)\n",
    "            o_s = self.transformer(o_i, s_i)\n",
    "            o_cl = self.transformer(o_i, cl_i)\n",
    "            o_temp = torch.cat((o_sem, o_c, o_ca, o_mg, o_na, o_s, c_cl), 0)\n",
    "            \n",
    "            s_sem = self.transformer(s_i, sem_i)\n",
    "            s_c = self.transformer(s_i, c_i)\n",
    "            s_ca = self.transformer(s_i, ca_i)\n",
    "            s_mg = self.transformer(s_i, mg_i)\n",
    "            s_na = self.transformer(s_i, na_i)\n",
    "            s_o = self.transformer(s_i, o_i)\n",
    "            s_cl = self.transformer(s_i, cl_i)\n",
    "            s_temp = torch.cat((s_sem, s_c, s_ca, s_mg, s_na, s_o, s_cl), 0)\n",
    "            \n",
    "            cl_sem = self.transformer(cl_i, sem_i)\n",
    "            cl_c = self.transformer(cl_i, c_i)\n",
    "            cl_ca = self.transformer(cl_i, ca_i)\n",
    "            cl_mg = self.transformer(cl_i, mg_i)\n",
    "            cl_na = self.transformer(cl_i, na_i)\n",
    "            cl_o = self.transformer(cl_i, o_i)\n",
    "            cl_s = self.transformer(cl_i, s_i)\n",
    "            cl_temp = torch.cat((cl_sem, cl_c, cl_ca, cl_mg, cl_na, c_o, c_s), 0)\n",
    "            \n",
    "            c_temp = torch.reshape(c_temp, (1, 7))\n",
    "            ca_temp = torch.reshape(ca_temp, (1, 7))\n",
    "            mg_temp = torch.reshape(mg_temp, (1, 7))\n",
    "            na_temp = torch.reshape(na_temp, (1, 7))\n",
    "            o_temp = torch.reshape(o_temp, (1, 7))\n",
    "            s_temp = torch.reshape(s_temp, (1, 7))\n",
    "            cl_temp = torch.reshape(cl_temp, (1, 7))\n",
    "            \n",
    "            c_predict = torch.cat((c_predict, c_temp), 0)\n",
    "            ca_predict = torch.cat((ca_predict, ca_temp), 0)\n",
    "            mg_predict = torch.cat((mg_predict, mg_temp), 0)\n",
    "            na_predict = torch.cat((na_predict, na_temp), 0)\n",
    "            o_predict = torch.cat((o_predict, o_temp), 0)\n",
    "            s_predict = torch.cat((s_predict, s_temp), 0)\n",
    "            cl_predict = torch.cat((cl_predict, cl_temp), 0)\n",
    "        \n",
    "        c_predict = c_predict[1:]\n",
    "        ca_predict = ca_predict[1:]\n",
    "        mg_predict = mg_predict[1:]\n",
    "        na_predict = na_predict[1:]\n",
    "        o_predict = o_predict[1:]\n",
    "        s_predict = s_predict[1:]\n",
    "        cl_predict = cl_predict[1:]\n",
    "            \n",
    "        output = torch.cat((c_predict, ca_predict, mg_predict, na_predict, o_predict, s_predict, cl_predict), 1)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cfa25b8-102a-4899-9502-11a4662765bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.get_layers = _get_layers(7)\n",
    "        self.feature_extraction = _feature_extraction()\n",
    "        # src_vocab = 211\n",
    "        # trg_vocab = 7\n",
    "        self._cross_transformer = _cross_transformer(device)\n",
    "        self.out = nn.Linear(49, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sem, c, ca, mg, na, o, s, cl = self.get_layers(x)\n",
    "        sem, c, ca, mg, na, o, s, cl = self.feature_extraction(sem, c, ca, mg, na, o, s, cl)\n",
    "        output = self._cross_transformer(sem, c, ca, mg, na, o, s, cl)\n",
    "        # output = output.to('cuda')\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bce648a1-cf45-4e71-b332-e3630bc1660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model_regression = TheModelClass(device)\n",
    "model_regression = model_regression.to(device)\n",
    "# print(model_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab00e1c-3303-419d-9c38-a5a1f7e7ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, elements):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_files = os.listdir(data_dir)\n",
    "        self.data_files.sort()\n",
    "        if self.data_files[0].startswith('.'):\n",
    "            self.data_files.pop(0)\n",
    "        \n",
    "        self.elements = np.load(elements)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.data_dir + '/' + self.data_files[idx]) # (1920, 2560, 10)\n",
    "        data = data/255\n",
    "        data = np.transpose(data, (2, 0, 1))\n",
    "        elements = self.elements[idx]\n",
    "        elements = np.float32(elements)\n",
    "        return torch.tensor(data), torch.tensor(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e70f6b14-b82f-41cc-b9c0-63e30c1baac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset('EDS_predict_path', 'elements.npy')\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba77a1e-c168-4190-a04b-d564e862c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = torch.load('regression_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770afdd1-c3c4-4007-b668-575cbda42471",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 47.54 GiB total capacity; 35.72 GiB already allocated; 3.69 MiB free; 40.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-de703258c1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# target = target.to('cuda')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-dc4f2e89b837>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cross_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# output = output.to('cuda')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f47605641453>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sem, c, ca, mg, na, o, s, cl)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mcl_sem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msem_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mcl_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mcl_ca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mcl_mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmg_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mcl_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8b5149643099>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, src_mask, trg_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0me_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0md_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4afaa0651c90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# x = self.pe(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2e620e5f5cb8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-aee792bbef5b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 47.54 GiB total capacity; 35.72 GiB already allocated; 3.69 MiB free; 40.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "targets = np.empty([0, 7])\n",
    "outputs = np.empty([0, 7])\n",
    "\n",
    "for source, target in dataloader:\n",
    "    source = source.to('cuda')\n",
    "    # target = target.to('cuda')\n",
    "    output = regression_model(source)\n",
    "    output = output.detach().cpu().numpy()\n",
    "    targets = np.concatenate((targets, target), axis=0)\n",
    "    outputs = np.concatenate((outputs, output), axis=0)\n",
    "    source = None\n",
    "    target = None\n",
    "    output = None\n",
    "\n",
    "print(targets.shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c14858-d1b7-4c0b-986c-8a518f584e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets.reshape((-1, 1))\n",
    "outputs = outputs.reshape((-1, 1))\n",
    "plt.scatter(targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affee2cd-dedf-4328-9859-d9ecd4c66b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_out = np.concatenate((targets, outputs), axis=1)\n",
    "with open('regression_model_evaluation.npy', 'wb') as f:\n",
    "    np.save(f, tar_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d7b8d13-c251-4721-91b6-e5317020c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.921100114508617\n",
      "4.158637115731935\n",
      "28.139343443219307\n",
      "17.3404472061916\n",
      "11.948959408163883\n",
      "4.308790093554549\n",
      "17.759018278162078\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    loss = LA.norm((targets-outputs)[:, i])\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02e00e16-4c88-430e-940d-9d6bd4f77493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFgAAAD8CAYAAADzGGPMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbvElEQVR4nO1deZBdZZX/nfde93vZ9wkRUmRPyNoJIRhRWWKAhIxxYdMZEaWGqVIcHVDIODWjU+NUSY3lglAoKg4oGhmEAZlIhglLBAcJKMOWQQMmkISkQxYk6eV19z3zx30dO537+3W/j/fCvTGn6la67+nvLr+c+33nnO8s5u44RvWj3Fv9AEc7HQO4znQM4DrTMYDrTMcArjMdA7jOVBeAzexcM3vBzDaZ2ap63CMrZLXWg80sD+C3AJYC2ApgA4APufvzNb1RRqgeErwIwCZ3f8ndywBWA1hZh/tkggp1uObxAF7p8ftWAKfKhygN8uLgkYm8zgF8XGl7K+V1DeMDu4r8mpFApGE/5x3Yt/U1dx/T+3w9AO4XmdnlAC4HgMZBIzBj5d8m/t2euXwKm/aPz1HeH5bOpLy90/KUVx4RUd7bHuG8R++6ekvS+XoAvA3A+B6/n1A5dwi5+00AbgKAAceN9/YRlngxizjAnQumUF5USL4eAFjgshPl+TUZ1QPgDQCmmtlExMBeDODDaoDngfKwZJ518ZfaM71EeQN2c2kzzoJ18vvlOsVAQjUH2N07zewKAGsB5AHc7O78W+4el08WKxdCoyTRhbRZl3iQGi/7dZmD3X0NgDX9HlCK4JNbknlbB9JhtnI3v+atyYsmAHQ18mGe4/9ratph9JYtcr3J2IsJMRXTM6yLM5XkW6RArH7yTgXAQ4ptOGPi7xJ5/71tHh1377ybKe/8/GeDnkVNSR4wfaQCYHdDZ5SsOqlFTpHSPhRQSrpzndU/RyoAbt01AM/cOCeZOYe/8cdPOodf8y85imqRU3Nw66jqRTgVAEd5gOrBQmo6TpkedD85Bws1LbNTBIw/vBfUYiUWwACjAIh1cn7D6q+XCoA9D7SPIHqwkJrdJ3FDQ94vDPsgSgXAwaSAErycMjRqTKkA2IsRosnEM7aDS2lhxWuU1/Gfhzm2DhJRWABoMzqEUgGwGWBk5VEv3N7BH78ejm5pYhNKBcCDG9tx+qRNibwHtxP1DcB9C75DecvvvzroWaIGpT9n1FRu3TUAT98wN5k5L0wPbv8Mv5+ag5Vh0zomowDDteVFKSeMCeVpUx46BX7A/JwKgKMGoGUsU4T5W3U2cYe78stoNyfnhVAqAIZri40OE+5DV28WcK9QSgXAXgDaRhOxEt/znulq9zLsWeR2UlYtOUC8mHhjF3Ow1NPkFBE4eRNKB8ClCNHU5B0N38G333PLuKGBx0dTll7kartZmgqAQ/Vg5XBf8rjQg5VT/Wh0uANAxN5MbuFwklZXoIYRsGOUDoBbdw3AszcQSQ00NNr+ma9ypV1CFAWIZeLxU5QKgAHwz1Z+s2Ebm/IxxLhILYCEUgFwVADaRhEghfnUeTLf0QiZL+NnORq1CAf/NNU7KRADnep6QzSjEuwFoH1k9Q//2iwReilMbAni0egPRilCNO1AIsu2DKLDorP3Up69SILdEO5NU/uD9F59/YGZ3WxmzWb2bI9zI83sfjP7XeXfEZXzZmbXVVIHnjazBVU/US/yvNNjQGMHPeQ7RfzwgtPj4FSWdBDqjwT/G4DrAdza49wqAOvc/cuVHIxVAK4BsAzA1MpxKoAb0UfwNRAbGqdN+H0ib/2OWXTc2rk/oLwFLyTHGwM6Nk1SPRY5d19vZhN6nV4J4IzKz7cAeAgxwCsB3Opx4sdjZjbczMa5+6vqHmaORhI2o7xsLVHg7qX60oVhI/0UhELn4LE9QNsBYGzl56T0geMBHAZw7wj3/72OxKA18Ye4bPZyznvkQcq77cdLKE+BeNGZv6S8a8n5N73Iubsb27HU4w5GuA8aPd7dkiVHhTJ5Jxfv/SIRI3SR6wrYSg0FeGf3p29m4wA0V873K32gN0V5bmioraSOU2dQ3rDCLyivU8WrCFnJBziZQwG+B8BHAXy58u/dPc5fYWarES9ur/c1/wKANwBtY6qP7Nkzg0tplxiYU0qGWMg6AvaT+gTYzH6MeEEbbWZbAXwBMbC3m9llALYAuLDy52sALAewCUALgI9V/US9KCeC8dRi1RKFqQrK0BiYL1d9vf5oER8irMNWior28Mmqn6IYwSclO9wj4XDPr9hDeSUhpuqrUItcW9TABxJKhyUHIJdPFh2liYXs9AN9LHJCTWsICO1JBcBDim04fULyjsa6l3kKwc+bvk95N+09mfJCPW0hlAqADUCRxeerKElxzVzI9gP6yDLKqrvywM6BePzrROLm8Ze6ZA43NPafzn3FubDAeDz6D28X3DsTz6YCYBXZo3yw5fmT+TVFUIoKX5ULYABaqQAY4OpRiP0P6K2ffLsYJyy5XDmrDvc80BbgcFcpBINf5Su+kkT1Hxo1ZHQOBoTEyS32sAQZZfHKRPGsRleiFMGnJ+9oqFzl/Hk8V9lvHsF5gRGUmQ48oSQU/3wubANN2gsysiejU4TKVV73chMdd8+cWyjvg7iS8lRJGUUhQeKpALileSCeur4pkedN/KUubfpzfs3z+fesfDbKlmgZXQdv2pGioORAJVGKFWgqZ3aRk5E9Aqly00TKk/nIoVt5AYtjKgAGQBcX5Q+2jrBFTgEl44OzKsEqskc5X16bK1S4jlBfZm1zCFIBMCDmYDE551TZGDENyMIaQi3MbKYnihFsMjE0XuFSCrGjEf1sFOXJBfVoLIo0pNiGJZN+m8hb+2oTHbdWONzPvUfU7AmMvAzJ0k8FwIpCV3zlkgylkJCrVADc0jwQT143P5Hn8/knqxzu7Z8Qc7eODeTXJGVvFKUCYDj3funEwLBcC2mEBFYaZJQKgKMGoHVMMlgulM/2+ZPCbqhAVLpuVvVgFWMrdxiEoSEXpMBFLrMSrEKnFO2ezXc0ghc55a7MrKksInuwnUf2FN+7i/La14+lPGVoSEdQSEmLvv7AzMab2YNm9ryZPWdmn66cr2kageU88XADPXLm9FDkeX7ogeIg1B8J7gRwlbv/2syGAHjSzO4HcClqlEYwtNiGpZNeSOT9fFuy+gYAa2bfRnnvePgqyguum1aPcgaV8NNXKz+/YWYbEUet1zSNgJESxjcCUwh0ALYYWO9FrpKrMR/Ar/Am0wh6pxBsuC55JjGebI+/UkWRvsjRGLgjLN0+ZCHuN8BmNhjATwF8xt3/YD1C/kPSCA5LISDzn3JXWgN//OCEQvEaVq/YNDNrQAzube7eHYRVszSCqCDMUPHC7aILgdIGAsJ840eph7PHYlH9HoCN7v7VHqyapRGERvbsOUltDwtJVGqailurU82e0wB8BMAzZvZU5dznUes0guq35PQ2uixlHZaWUBdDw90fAX/92qQRlCJgarLD3beJLgTLeWSPbeQOd+lTqG2N/JRYcpC1NcQYkZUZWPpWLo5ZLes1uLEd75rwUiLv4W2z6bh7mr5HeWc+/znKC42LyG6R/OYBeO6bBEhhaF8+j0f2XPDAo5T3sx+9k/IU+Oec8wTlbfp88vlUAAzjG4ouI6lFsmFg0R41RdA8EkGpAFh2IRC6Z/vcCZwX8R3nUH/wgYCowVQAHEsw4clAEE4qKzPUJTlI5R4QSgXAsmaPeOE9M7lEtQRX3eCU2TQuQBUHFYME+CorU82zR2f94GIEJ5E9vp0bGqUVO4Nu9yfXhUDqwVu5HnzfnB9S3rW7uI9flzPgrEJAykIqADZzFJgXXEwRHaI2WrtoLxuSUAgAnVFGm0W1NA/Eb25oSmaKblyXLHw/5b1+uoiZCAyneOzaRYJ7e+LZVAAMgEqqrNJH6vzEA0Wtn8AawZlNgglNIWiZN57yoMAXFNoeglEqAAbEwx/B2g4A+vA/V3+5VAAc91UmbyZeat9UbkwMbK6xvoUMh06hFCE3gzSO38qLg5bO43qwfVcUyQ8sZxASjpUOgMHXJOXs6ezib9wg8jdC/Lqh41IB8NBiG86e9H+JvDWv8No7GxYkq0YA8M78X/Mbqk9dSXCWC3IwUp9sh6sQnbBr1ppSAfCB5oF47PqFycy5XNxWzn4P5bVeyFGUAKsGgcMzaskBfK6VBTKKXIsILSMuPW1Z1YOjgmiGJ3QjZWiE1oSoNaUCYNWNS0nNnplcghv21z4TP7t6MGrvcJfzbKAEZ1cPLkbITUs2NOxlbmgMW8ZD3g7ceRzl6VRawavHHGxmJQDrARQrf3+Hu3/BzCYCWA1gFIAnAXzE3ctmVkRcUP9kALsBXOTum/VNnLb9VWComj0y300tcsqYqNMU0Q7gLHffXwljfcTMfg7gSgBfc/fVZvYtAJchThe4DMBed59iZhcjLm9+kbrB4IYyThuf3IVg3XaxozHzDspbfOffyJcKoRBnT5+KncfU/f02VA4HcBaA7je8BcD7Kj+vrPyOCn+JScdtvKORsyjxsE6jh7xmFz/k++b4EUL9DcDOI54GpgC4AcCLAPa5e/cH1Z0mAPRIIXD3TjN7HfE08lqvax6SQvDrG5uS7z2Lf5fvnyvanV2u5gjBElJat77K7t4FoMnMhgO4CwCvTt9P6plCMOC48d4+svouBC2LeHHQ4DDU0MhLQlVpEe6+z8weBLAYwHAzK1SkuGeaQHcKwVYzKwAYhnixExcOtOQCq07V2lpT1B8tYgyAjgq4AwAsRbxwPQjgfMSaRO8Ugo8C+J8K/4FKUDYlLwBto1hkD39jZWgoSZQhZjV2BPVHgscBuKUyD+cA3O7u95rZ8wBWm9mXAPwGcR4HKv/+wMw2AdgD4OI+72DCCa6yjFRFKiGlsplAjaN++pNC8DTi3Lje518CcNg+tru3AbigqqcoRshNSTY0oi2D6bChK7ihsfuhcZQna6odjRHuilQPty6VQqBqdYRWuc6qL2JosQ3vmZhcFOm+V3iu8kNz/p3ymh7+FOWp0CnrOArLeh1oHojHrye5AtyQw8oFyyivfBUXt8Z9YapC25iMxqYBfF6UNdxFNy55L4WTWlRF9RVGqQA4KgAtYwMie4ShIWuxiwJzsq5lVrvSAmGGhiSV6Bk4B4ekM6QCYC+ElQrYN1VldfP/Gbn9rkDMqhYBgD68bskgeIGbnrr0LecxSgfAxQj5qcTQEDsaQ87ZQXktT/OiSKHb9iGUCoCHFtuwdCKJ7HmZR/asm/MTypv1rNCDA7SBUEoFwJEbDnQme2DqUcAo9Jp9OfmTKBUAt+4agGduJMV55vGV5QOLVlLeB+99jPLuvf0dlKfm5w+fu57yvnR18vlUAAzwF5MLUpnrW6rlpKo6pRa5joB9+1QAHBWAttHVGxqtJ0+gvLaIVwUMTaU9kn2Va05MODQYHA2V6Rm6gZnZnp6eB8rDA4qDzqp9PrKaIvIBZmUqAEYpgk9JTqU1UbNn6NlcD5YSHFiEOdttf4mJGqr3hzauVnPwsHxr1ZdLBcBDim04i3TjWru1iY77j1k/oLyvvraY8uROtZhm92e1IEcOjkaSYaI+2YHGP1lZUia0zUNA/GoqAN6/axAeveGUZOZJfNwFC3lRpH3vFo2kVK6yAP/hf+IGShyPczilAmBJQmpUuIUKSgmNoMx1ZlSCpaEh3qnl5BMpL6Q9JAApwZntShvlRWSPoH2T+Rys2v7KDgWhtTIJpQJgANzhLvbWukphUhpa3jaEqinQnAfwBIBt7r6ilhHuuVIXGqf+IZHXvnkIHXfcua9QXsu3j6c8WVKmxl1pq7HKPw1gY4/fr0Uc4T4FwF7Eke1Ajwh3AF+r/J0kd0MU5RIPREaP1o4Geljk9IjyoId1GD1yHU4PRv0NwD4BwHkA/gXAlZWI9bMAfLjyJ7cA+CLiFIKVlZ+BOML9ejMzFWEpdzS2hO1oLPErKE8W3RChWvVsXP11AFcD6P5eR+FNRrj3pBw8qC5kl1TTqr5cn1SvEuMrADS7+5Nmdkb1t6DXPZhC0DB4BB741tuT/3A6B/GCU99Hefvfz91icpET0+z+t9XH4X4agPea2XIAJQBDAXwDbzLC/ZAUgnHjvTyc6cEismeOWMjKfFy+PazEeIgE9yfL6O/c/QR3n4A4mPoBd/8L/DHCHUiOcAf6GeEOgLbTCRnjhlgSyREV+KF7+4AfhN6MHnwNahThHuVFcVBB+6bwbz20VaXso1Hv4D93fwhxS53aRrgD/NMMDLJWqlPoNUMoFZacFSMUJidH9nRsEZE9y3hRpNa7eGRPaL2IEEoFwMOLrVgx+dlE3l2biXYB4JG5d1Le/Hs+QXmhtSvrqQfXlRxAB9ttFNGO7c5tXumuVHNpYHsIRqkA+I1dg/DIt4jDfYaI7BF6cNulHA2ZxiWolcZucEoFwObCCR5Wwl1SrdtKKkoFwFED0BZSs2f22/hFAw0Gi7iUZrf1uoMDIl5Y5rsF1mJXyedRPqNThBeA9hHJL6akTRkaqiNDqCqW2dKKKHWhMO2NRFZ5M0+lHbSc68H71vOaPabaYdTYCEkFwAYe2aNUo8a8ikPlrNCgn8zWDx7a2Ib3nJjc9vfeF4n6BuDBWXdT3sz13NCALFonHO5ZnYMBIM+i0AJb7UhNIbC8bWbTuPY3D8Ivvkn6XoguBMvnn015Kld5wM6w3egyWYgVpQJgN5EcKNQ0K/DHjxoCC/MIymzlv7hZFCmKlOcry4EmvqMRGuAnXaABJnYqAI5EcVAF1N5pgam0YpjOxBc8QqkAGA6Kh3xhZQ6LhSx0kcusmpYrdWHAjH2JvJbfDafjRp7HG44feJrX7JGSeDQaGu6Gjo7kR5F5cmpXVC2OMk9O5XFlVA8eVmzF8knPJfLu3sIbNK2dxWv2zHqG5yqriV39h2bXmwaR5CeERnYhCPTryt2OzBoaKoVAFAe9aPH5lHfxz3hf5btvexflKf35/DN5/vNXViWfTwXAsmaPiFnZP4873BssucMiAF0AVEgpNecFpQJggM9v6pPNdfAXppuo6CMoRQCsrskoFQC7iuwR0qZSaVXaa60rrCpKB8AQgT3Cm6bch7l67JYGUH8DsDcDeAOxJ7XT3Rea2UgAPwEwAcBmABe6+95KcPY3ACwH0ALgUnf/tbq+SiEovziUjhu2hOcqq6apOgmG/6fVu3H1me7eM4h6FYB17v5lM1tV+f0aAMsATK0cpyKOeuc9eCvEqrwrCVZteEMCugFI//ORzvRcCeCMys+3IA4KvKZy/tZKyOpjZjbczMa5O61FO7SxDeecuDGRd/eL/P9GGRr/upuUqEEfu9FCSOtZL8IB/JfFzS6+XQmeHtsDtB0AuqPtDqYQVKg7veAQgA8pkj9wBG+pu4hLzXvH87g1X8wBNr4LhZwofPTEJ3glWCA5Tq6/AL/T3beZ2Z8BuN/MDslYcXc31mmEUM8I90Gjxjt1ZguRMpFWpWK+1aanmpLqVrvS3bdV/m02s7sQxwXv7P70zWwcgObKn3enEHRTz/SC5OsXgNbRzFQWft1TZvGLitYdORUzIfRuD2gl3KcDzswGmdmQ7p8BnA3gWRyaKtA7heASi+ntAF5X8+/B+3jyofMEOMlhKhVApB7kOiN6MOqPBI8FcFelmUsBwI/c/T4z2wDgdjO7DMAWABdW/n4NYhVtE2I17WN93SDe0SBMMfPsmcnLzYzc2NLXbRNJhk411KHqVCVVYF7C+d0AliScdwCfrOopxI6GTG1VxUHzHAy5eSkL+mTUH+ylCOVpyfVwcttKdFzL0uS0AwAY9SwHWOcqCz24nNGyXoOLZSye9PtE3q+28ZInPznlO5T3WfC2v5EovaPCV6NChgvTMZKlwuX+GecVxPQs1bSujLor25tLeOm65P5Tzm0JrJp6GuW1fIBPLaH1IlqO54sqo1QA3K2NJVFoQqFKggmJ0AmldACcB9pGkPlNAOULZ1KeKmBUaOPPIlMIhL5L71X1iHqQBeauBdTQAYCuwPjgTBdFah+RzFOL3J7ZPPq9tI9LW7AnU6XnEkoFwF6KUJ6SrAfbDr5Yta5IdtIDQOmHHPzQfOS6tDs7UpQjkqrUps7OQKSUdSh5GZ0iBhfbsXhi8jb7L7dxj9nqU75LeZ/6MW/7q/PkOC9fzugi5zB0Mt0ppKE89DSgOsHI/nXFjFpy5Z0lbL5uejKTh6ZJQ6P1skBfhCrlyHzWglIBsJuQOBUDXOROBRl4qSw5tYOS1dg0uaMhwpV8Jq9TW+venEBY4HYqAI5r9hCmkKjdc7gqJquaqOj3GufOpALgg9sySazA8i+68Z4YV+OwqlQA7EVhaLwqDI2Vr1Necc0wfr/A6MrMphBIEoZGRwcX08bAUgdSgrO6yKkdjcd2JvuJAeCni26ivEvWXclvKOvy1HYSTgXADr47oSSqKzDXQvPCrskoFQArQ0OFOX1ukjA0rlGB2/yaynvXNiajhkbUALSMTX54FxGUEA53RbpRyVGYJwfwlw6uxCemFmkqCwpR4VIBcJQH2pihIV5KOdzVii873ar/mCznyVFJFYtO8L0CdzTqJsFmNhzAdwHMRiwbHwfwAmqUQuClCOWZJFhh2wA6bv8yHtmTf5J3Lwit2ROy3d/fafsbAO5z9xmI49Q24o8pBFMBrKv8DhyaQnA54hSCPsm7LPGgYZfmyOUieqjoSusSB78d8mV+MOpPDfdhAN4N4FIAcPcygLKZ1SyFYGBjB+ZPSO6J8dSOqfTZbj+Z72hctOEqygstrVivzuATAewC8H0zm4e4KcmnUeMUgu03TEm8uS3gD3bVFJ4SO2kdz/R8+aeqHRenE1ZsprznvpJ8vj9TRAHAAgA3uvt8AAfwx+kAwMGQ1apTCNx9obsvbCjxIsyB8dcoWBc9PA96yNrvbvSgz9EPLLYC2Oruv6r8fgdigGuWQhAVgBZmJamMTWFodHoz5SlSlpysT0GoP10IdgB4xcy6bdklAJ5HLVMIPMYx+TB+dDk9FKlFTrX2CaH+6sGfAnCbmTUCeAlxWkAOtUwhGFXlkwPYLQyNsYfkTB5KapFTC1leme2E+ptl9BSAhQms2qQQANRii8Q2uooVC+3p6bLeWvWUCkvOSxE6picbGrY9LIWgM1Ied/EwKsK9zrnKdaMhxXa8a9KLibz1O3hkzx2ncIf73295H+XVuleGolQA7DB0kMge5U3rEEiVRZS1srzknlyABFs/2gzVncxsF+KFsptGQ7RHO4JUzXOc6O5jep9MBcC9ycyecPekRTVzz3EEZ6M/TToGcJ0prQBz9eDI0pt+jlTOwUcTpVWCjxpKFcBmdq6ZvWBmmyqFlt7KZ9lsZs+Y2VNm9kTwddIyRVQ6j/8WwFLELtINAD7k7s+/Rc+zGcDCXpW2qqY0SfAiAJvc/aXKttRqxNtPmaY0Acy2mt4q6q609WRleyuIUuGLSCkdVmnL3ddXe5E0SXDVW031pJ6VtgB0V9qqmtIE8AYAU81sYmXn5GLE209HnESlraopNVOEu3ea2RUA1gLIA7jZ3ZMLu9efEitthVwoNWra0UppmiKOSjoGcJ3pGMB1pmMA15mOAVxnOgZwnekYwHWmYwDXmf4fh2mG8PIz/tUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff = targets - outputs\n",
    "plt.imshow(diff, interpolation='none', aspect=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0658f00f-1d32-41bb-94e6-105f2276b585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets_outputs = np.concatenate((targets, outputs), axis=0)\n",
    "\n",
    "plt.imshow(targets, interpolation='none', aspect=0.025)\n",
    "plt.show()\n",
    "plt.imshow(outputs, interpolation='none', aspect=0.025)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "240df110-4ecb-41ea-a439-12bb066b7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.fromarray(targets)\n",
    "im.show()\n",
    "if im.mode != 'RGB':\n",
    "    im = im.convert('RGB')\n",
    "im.save(\"targets.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f35ae-7600-48ac-bbf2-1300a91e0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
